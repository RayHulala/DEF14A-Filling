{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Import modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import StanfordTokenizer\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info = pd.read_excel('info.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info['share_indicator'] = np.nan\n",
    "info['num'] = np.nan\n",
    "info['person'] = np.nan\n",
    "info['num_person'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "share_num = {}\n",
    "token_dict = {}\n",
    "pos_dict = {}\n",
    "ent_dict = {}\n",
    "share_num_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = StanfordTokenizer()\n",
    "\n",
    "pos= StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "\n",
    "parser = StanfordParser(model_path=u'edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')\n",
    "\n",
    "dp = StanfordDependencyParser(model_path=u'edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')\n",
    "\n",
    "ner = StanfordNERTagger('english.conll.4class.distsim.crf.ser.gz') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Joblib paralell cumputing function for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallel(function, loop):\n",
    "    core = multiprocessing.cpu_count()\n",
    "    Parallel(n_jobs= core, backend='threading')(delayed(function)(i) for i in loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Find out data with \"pledg\" but not share pledging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating indicator for \"share pledging\" and tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_share(i):\n",
    "    text = info['infoclean2'].loc[i]\n",
    "    if 'share' in text:\n",
    "        info['share_indicator'].loc[i] = 1\n",
    "        words = tokenizer.tokenize(text)\n",
    "        token_dict[i] = words\n",
    "    else:\n",
    "        info['share_indicator'].loc[i] = 0\n",
    "        token_dict[i] = ['empty']\n",
    "    return print(i, 'find')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Find numbers near \"share\" and find name entities near such numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still cannot deal with parallel structure well; A, B and C pledge 1, 2, 3 shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_person(i):\n",
    "    token = token_dict[i]\n",
    "    tags = pos_dict[i]\n",
    "    entities = ent_dict[i]\n",
    "    number = [s[0] for s in tags if s[1] == 'CD' and ('shares' and 'pledg') in ' '.join(token[token.index(s[0])-3:token.index(s[0])+5])]\n",
    "    name = [t[0] for t in entities if t[1] == 'PERSON']\n",
    "    info['num'].loc[i] = ';'.join(list(number))\n",
    "    info['person'].loc[i] = ';'.join(list(name))\n",
    "    share_num[i] = []\n",
    "    for n in number:\n",
    "        for p in name:\n",
    "            if p in token[token.index(n)-5:token.index(n)+5]:\n",
    "                share_num[i].append((p, n))\n",
    "    info['num_person'].loc[i] = ';'.join(list(share_num[i]))\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 store results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def num_person(i):\n",
    "    if info['share_indicator'].loc[i] == 1:\n",
    "        find_person(i)\n",
    "    else:\n",
    "        share_num[i] = 'None'\n",
    "    return print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Computing and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 find\n",
      "1 find\n",
      "0 find\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "parallel(find_share, range(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag(i):\n",
    "    token = token_dict[i]\n",
    "    pos_dict[i] = pos.tag(token)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parallel_1(tag, range(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parallel_1(find_share, range(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('token.pickle', 'wb') as f:\n",
    "# Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(token_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in info.index:\n",
    "    token = token_dict[i]\n",
    "    pos_dict[i] = pos.tag(token)\n",
    "    ent_dict[i] = ner.tag(token)\n",
    "    print (i, 'nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tag.pickle', 'wb') as g:\n",
    "    pickle.dump(pos_dict, g, pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('ent.pickle', 'wb') as h:\n",
    "    pickle.dump(ent_dict, h, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parallel(num_person, info.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info.to_excel('detailinfo.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### merge with column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
